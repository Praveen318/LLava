{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-29T00:50:44.823537Z","iopub.status.busy":"2024-04-29T00:50:44.822815Z","iopub.status.idle":"2024-04-29T00:50:45.835841Z","shell.execute_reply":"2024-04-29T00:50:45.834707Z","shell.execute_reply.started":"2024-04-29T00:50:44.823504Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Mon Apr 29 00:50:45 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P0              25W / 250W |      0MiB / 16384MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["! nvidia-smi"]},{"cell_type":"markdown","metadata":{},"source":["## **Finetunning the llava model(VSFT on the llava-hf/llava-1.5-7b-hf model with 260k image and conversation pairs from the HuggingFaceH4/llava-instruct-mix-vsft dataset.) with custom dataset**"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:48:40.596131Z","iopub.status.busy":"2024-04-29T01:48:40.595529Z","iopub.status.idle":"2024-04-29T01:49:48.287290Z","shell.execute_reply":"2024-04-29T01:49:48.285987Z","shell.execute_reply.started":"2024-04-29T01:48:40.596100Z"},"trusted":true},"outputs":[],"source":["%%capture\n","!pip install -U \"transformers>=4.39.0\"\n","!pip install peft bitsandbytes\n","!pip install -U \"trl>=0.8.3\"\n","!pip install deep_translator"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:49:48.289551Z","iopub.status.busy":"2024-04-29T01:49:48.289239Z","iopub.status.idle":"2024-04-29T01:50:05.929543Z","shell.execute_reply":"2024-04-29T01:50:05.928685Z","shell.execute_reply.started":"2024-04-29T01:49:48.289519Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-04-29 01:49:55.284039: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-29 01:49:55.284163: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-29 01:49:55.418581: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoProcessor, TrainingArguments, LlavaForConditionalGeneration, BitsAndBytesConfig\n","from trl import SFTTrainer\n","from peft import LoraConfig\n","from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:18:23.171245Z","iopub.status.busy":"2024-04-29T01:18:23.170693Z","iopub.status.idle":"2024-04-29T01:18:23.175460Z","shell.execute_reply":"2024-04-29T01:18:23.174456Z","shell.execute_reply.started":"2024-04-29T01:18:23.171219Z"},"trusted":true},"outputs":[],"source":["# model_id = \"llava-hf/llava-1.5-7b-hf\"\n","model_id = \"HuggingFaceH4/vsft-llava-1.5-7b-hf-trl\""]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:18:23.178270Z","iopub.status.busy":"2024-04-29T01:18:23.177941Z","iopub.status.idle":"2024-04-29T01:18:24.423920Z","shell.execute_reply":"2024-04-29T01:18:24.422907Z","shell.execute_reply.started":"2024-04-29T01:18:23.178240Z"},"trusted":true},"outputs":[],"source":["quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:18:24.425474Z","iopub.status.busy":"2024-04-29T01:18:24.425111Z","iopub.status.idle":"2024-04-29T01:23:41.866409Z","shell.execute_reply":"2024-04-29T01:23:41.865436Z","shell.execute_reply.started":"2024-04-29T01:18:24.425441Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9bd93cbb3b845e282c2bb41f696bd49","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/971 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"069ff02acb4747e99bfdfdf6b457f69d","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/70.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"90bff2015e6d4828a9c8f0bd27367ed2","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d68a54f49e0d44bf80ef8d0170aa2d79","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00006.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f173aaab348c4f68b9aa92dd88c83191","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b363cbda8aa94c49bfb311c4693e34fa","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac5e1718c5f3428daa13ad54f017e72b","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4bcf336209c4bf19ffbc2efd2b8b6a1","version_major":2,"version_minor":0},"text/plain":["model-00005-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02623177b57b4cae912891061c07adb8","version_major":2,"version_minor":0},"text/plain":["model-00006-of-00006.safetensors:   0%|          | 0.00/3.94G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2422b9a991b24d8f89a24547e0903f48","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a56ecc533b924fe4b60d7d784e24b0fa","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/136 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model = LlavaForConditionalGeneration.from_pretrained(model_id,\n","                                                      quantization_config=quantization_config,\n","                                                      torch_dtype=torch.float16)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:23:41.868120Z","iopub.status.busy":"2024-04-29T01:23:41.867744Z","iopub.status.idle":"2024-04-29T01:23:41.872914Z","shell.execute_reply":"2024-04-29T01:23:41.871698Z","shell.execute_reply.started":"2024-04-29T01:23:41.868092Z"},"trusted":true},"outputs":[],"source":["LLAVA_CHAT_TEMPLATE = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. {% for message in messages %}{% if message['role'] == 'user' %}USER: {% else %}ASSISTANT: {% endif %}{% for item in message['content'] %}{% if item['type'] == 'text' %}{{ item['text'] }}{% elif item['type'] == 'image' %}<image>{% endif %}{% endfor %}{% if message['role'] == 'user' %} {% else %}{{eos_token}}{% endif %}{% endfor %}\"\"\""]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:23:41.874391Z","iopub.status.busy":"2024-04-29T01:23:41.874113Z","iopub.status.idle":"2024-04-29T01:23:44.430517Z","shell.execute_reply":"2024-04-29T01:23:44.429536Z","shell.execute_reply.started":"2024-04-29T01:23:41.874368Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cebd791f051644f5af1bd7b22c5423c9","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c471b63b84484b34bf5aed498c951846","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2162a9fe458e418994db9afe42237b06","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"60f15e6488c444fb98ffdbac3053c81b","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0855656738546f9b786823a3841f84f","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e0af5b22002c48d6876fb7e74c19aeec","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/819 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_id)\n","tokenizer.chat_template = LLAVA_CHAT_TEMPLATE\n","processor = AutoProcessor.from_pretrained(model_id)\n","processor.tokenizer = tokenizer"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:23:44.432730Z","iopub.status.busy":"2024-04-29T01:23:44.432093Z","iopub.status.idle":"2024-04-29T01:23:44.448050Z","shell.execute_reply":"2024-04-29T01:23:44.446797Z","shell.execute_reply.started":"2024-04-29T01:23:44.432695Z"},"trusted":true},"outputs":[],"source":["from PIL import Image\n","import requests\n","import numpy as np\n","import ast\n","import json\n","\n","class LLavaDataCollator:\n","    def __init__(self, processor, image_size=(640,480)):\n","        self.processor = processor\n","        self.image_size = image_size\n","\n","#     def resize_image(self, image_path):\n","#         image = Image.open(image_path)\n","#         image = image.resize(self.image_size, Image.ANTIALIAS)\n","#         return image\n","\n","    def __call__(self, examples):\n","        texts = []\n","        images = []\n","        for example in examples:\n","#             messages = [{'content': [{'index': None, 'text': 'How many computer monitors are on the wooden desk?\\n', 'type': 'text'}, {'index': 0, 'text': None, 'type': 'image'}], 'role': 'user'}, {'content': [{'index': None, 'text': 'There are two computer monitors on the wooden desk.', 'type': 'text'}], 'role': 'assistant'}, {'content': [{'index': None, 'text': 'Can you describe the position of the chair and the person in relation to the wooden desk?', 'type': 'text'}], 'role': 'user'}, {'content': [{'index': None, 'text': 'The person is sitting in the chair in front of the wooden desk, using a laptop computer. Their feet are up on the desk, and there are other computers in the background.', 'type': 'text'}], 'role': 'assistant'}, {'content': [{'index': None, 'text': 'What is the person doing with the laptop computer?', 'type': 'text'}], 'role': 'user'}, {'content': [{'index': None, 'text': 'The person is using the laptop, possibly working or engaging in some recreational activity. They appear to be in a relaxed, lounging position with their feet up on the desk.', 'type': 'text'}], 'role': 'assistant'}, {'content': [{'index': None, 'text': 'How many computers can be seen in the image? Is there an additional computer aside from the laptop?', 'type': 'text'}], 'role': 'user'}, {'content': [{'index': None, 'text': 'There are a total of three computers visible in the image. These include the laptop being used by the person, and two additional monitors on the wooden desk.', 'type': 'text'}], 'role': 'assistant'}, {'content': [{'index': None, 'text': \"Considering the presence of the laptop computer and multiple monitors, what could be the possible implications for the person's work or personal preferences?\", 'type': 'text'}], 'role': 'user'}, {'content': [{'index': None, 'text': \"Given the presence of the laptop computer and multiple additional monitors, it is likely that the person either has a demanding job or tasks that require them to multitask across multiple screens, or they have a personal preference for an extended workspace to enhance their productivity or workflow.\\n\\nHaving multiple monitors can help improve efficiency and work quality in various ways, such as managing multiple open applications, web pages, and documents simultaneously without having to constantly switch back and forth between them on a single screen. People working in fields like programming, design, video editing, gaming, or finance might especially benefit from this setup, as it allows them to access and compare different sets of data or work on multiple tasks at the same time with ease.\\n\\nAdditionally, having a laptop computer in addition to the multiple monitors could indicate a need for mobility or flexibility in the person's work environment. They might travel frequently or work remotely, in which case a laptop would be essential. Having a laptop on hand also provides a convenient backup device should any technical issues arise with the other computers.\\n\\nAt a personal level, the person might simply enjoy the versatility and workspace expansion that comes with using a laptop alongside multiple monitors. For someone who spends a significant amount of time in front of screens, be it for work, communication, or leisure activities, such a setup can help create a comfortable and efficient workspace that caters to their needs and preferences.\", 'type': 'text'}], 'role': 'assistant'}]\n","#             messages = {'content': [{'index': None, 'text': 'How many computer monitors are on the wooden desk?\\n', 'type': 'text'}, {'index': 0, 'text': None, 'type': 'image'}], 'role': 'user'}, {'content': [{'index': None, 'text': 'There are two computer monitors on the wooden desk.', 'type': 'text'}], 'role': 'assistant'}, {'content': [{'index': None, 'text': 'Can you describe the position of the chair and the person in relation to the wooden desk?', 'type': 'text'}], 'role': 'user'}, {'content': [{'index': None, 'text': 'The person is sitting in the chair in front of the wooden desk, using a laptop computer. Their feet are up on the desk, and there are other computers in the background.', 'type': 'text'}], 'role': 'assistant'}, {'content': [{'index': None, 'text': 'What is the person doing with the laptop computer?', 'type': 'text'}], 'role': 'user'}, {'content': [{'index': None, 'text': 'The person is using the laptop, possibly working or engaging in some recreational activity. They appear to be in a relaxed, lounging position with their feet up on the desk.', 'type': 'text'}], 'role': 'assistant'}, {'content': [{'index': None, 'text': 'How many computers can be seen in the image? Is there an additional computer aside from the laptop?', 'type': 'text'}], 'role': 'user'}, {'content': [{'index': None, 'text': 'There are a total of three computers visible in the image. These include the laptop being used by the person, and two additional monitors on the wooden desk.', 'type': 'text'}], 'role': 'assistant'}, {'content': [{'index': None, 'text': \"Considering the presence of the laptop computer and multiple monitors, what could be the possible implications for the person's work or personal preferences?\", 'type': 'text'}], 'role': 'user'}, {'content': [{'index': None, 'text': \"Given the presence of the laptop computer and multiple additional monitors, it is likely that the person either has a demanding job or tasks that require them to multitask across multiple screens, or they have a personal preference for an extended workspace to enhance their productivity or workflow.\\n\\nHaving multiple monitors can help improve efficiency and work quality in various ways, such as managing multiple open applications, web pages, and documents simultaneously without having to constantly switch back and forth between them on a single screen. People working in fields like programming, design, video editing, gaming, or finance might especially benefit from this setup, as it allows them to access and compare different sets of data or work on multiple tasks at the same time with ease.\\n\\nAdditionally, having a laptop computer in addition to the multiple monitors could indicate a need for mobility or flexibility in the person's work environment. They might travel frequently or work remotely, in which case a laptop would be essential. Having a laptop on hand also provides a convenient backup device should any technical issues arise with the other computers.\\n\\nAt a personal level, the person might simply enjoy the versatility and workspace expansion that comes with using a laptop alongside multiple monitors. For someone who spends a significant amount of time in front of screens, be it for work, communication, or leisure activities, such a setup can help create a comfortable and efficient workspace that caters to their needs and preferences.\", 'type': 'text'}], 'role': 'assistant'}]\n","#             m = example[\"messages\"]\n","#             d = json.loads(m)\n","#             messages = [ { \"content\": [ { \"index\": null, \"text\": \"Who wrote this book?\\n\", \"type\": \"text\" }, { \"index\": 0, \"text\": null, \"type\": \"image\" } ], \"role\": \"user\" }, { \"content\": [ { \"index\": null, \"text\": \"Blue Star Coloring\", \"type\": \"text\" } ], \"role\": \"assistant\" }, { \"content\": [ { \"index\": null, \"text\": \"What is the title of this book?\", \"type\": \"text\" } ], \"role\": \"user\" }, { \"content\": [ { \"index\": null, \"text\": \"Adult Coloring Book: Stress Relieving Animal Designs Volume 2\", \"type\": \"text\" } ], \"role\": \"assistant\" }, { \"content\": [ { \"index\": null, \"text\": \"What is the genre of this book?\", \"type\": \"text\" } ], \"role\": \"user\" }, { \"content\": [ { \"index\": null, \"text\": \"Humor & Entertainment\", \"type\": \"text\" } ], \"role\": \"assistant\" }, { \"content\": [ { \"index\": null, \"text\": \"Is this a comedy book?\", \"type\": \"text\" } ], \"role\": \"user\" }, { \"content\": [ { \"index\": null, \"text\": \"Yes\", \"type\": \"text\" } ], \"role\": \"assistant\" }, { \"content\": [ { \"index\": null, \"text\": \"Is this a historical book?\", \"type\": \"text\" } ], \"role\": \"user\" }, { \"content\": [ { \"index\": null, \"text\": \"No\", \"type\": \"text\" } ], \"role\": \"assistant\" } ]\n","#             messages.append(d)\n","#             messages = [{\"content\": [{\"index\": null, \"text\": \"Who painted this portrait?\", \"type\": \"text\"}, {\"index\": 0, \"text\": null, \"type\": \"image\"}], \"role\": \"user\"}, {\"content\": [{\"index\": null, \"text\": \"John Singleton Copley\", \"type\": \"text\"}], \"role\": \"assistant\"}, {\"content\": [{\"index\": null, \"text\": \"When was this portrait created?\", \"type\": \"text\"}], \"role\": \"user\"}, {\"content\": [{\"index\": null, \"text\": \"Between 1760 and 1770\", \"type\": \"text\"}], \"role\": \"assistant\"}, {\"content\": [{\"index\": null, \"text\": \"Identify the technique used in the creation of this portrait.\", \"type\": \"text\"}], \"role\": \"user\"}, {\"content\": [{\"index\": null, \"text\": \"Oil on canvas\", \"type\": \"text\"}], \"role\": \"assistant\"}, {\"content\": [{\"index\": null, \"text\": \"What is the name of the subject featured in the portrait?\", \"type\": \"text\"}], \"role\": \"user\"}, {\"content\": [{\"index\": null, \"text\": \"Nathaniel Hurd\", \"type\": \"text\"}], \"role\": \"assistant\"}, {\"content\": [{\"index\": null, \"text\": \"Describe the artistic style of this portrait.\", \"type\": \"text\"}], \"role\": \"user\"}, {\"content\": [{\"index\": null, \"text\": \"The style of the portrait is 18th-century American painting.\", \"type\": \"text\"}], \"role\": \"assistant\"}]\n","            \n","            messages = example[\"messages\"]\n","#             print(type(messages))\n","#             print(messages)\n","            text = self.processor.tokenizer.apply_chat_template(\n","                messages, tokenize=False, add_generation_prompt=False\n","            )\n","            texts.append(text)\n","#             url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n","#             image = Image.open(requests.get(url, stream=True).raw)\n","#             print(example[\"images\"]) \n","            image = Image.open(example[\"image\"])\n","#             image = Image.open(\"/kaggle/input/images/images/1915.534_web.jpg\")\n","#             resized_image = self.resize_image(example[\"images\"])\n","#             print(example[\"images\"][0])\n","#             images.append(example[\"images\"][0])\n","#             print(image)\n","            images.append(image)\n","\n","        batch = self.processor(texts, images, return_tensors=\"pt\", padding=True)\n","#         print(batch)\n","        labels = batch[\"input_ids\"].clone()\n","#         print(labels)\n","        if self.processor.tokenizer.pad_token_id is not None:\n","            labels[labels == self.processor.tokenizer.pad_token_id] = -100\n","        batch[\"labels\"] = labels\n","#         print(batch)\n","        return batch\n","\n","# Example usage:\n","# processor = ...  # Initialize your processor\n","data_collator = LLavaDataCollator(processor)\n"]},{"cell_type":"markdown","metadata":{},"source":["### **Importing Training and Test Dataset**"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:23:44.449557Z","iopub.status.busy":"2024-04-29T01:23:44.449261Z","iopub.status.idle":"2024-04-29T01:23:46.353560Z","shell.execute_reply":"2024-04-29T01:23:46.352338Z","shell.execute_reply.started":"2024-04-29T01:23:44.449528Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"700d52555ec7418a86502aaf4d950e38","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4784bc85cbbe4f41b4465d6e9b3fdd4e","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","train_dataset_2 = load_dataset(\"/kaggle/input/train-5\")\n","test_dataset_2 = load_dataset(\"/kaggle/input/test-5\")"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:23:46.357367Z","iopub.status.busy":"2024-04-29T01:23:46.357013Z","iopub.status.idle":"2024-04-29T01:23:46.371772Z","shell.execute_reply":"2024-04-29T01:23:46.370821Z","shell.execute_reply.started":"2024-04-29T01:23:46.357334Z"},"trusted":true},"outputs":[],"source":["train_dataset_1 = train_dataset_2[\"train\"]\n","# specific_train_dataset = train_dataset_2[\"train\"]\n","# train_dataset_1 = specific_train_dataset.shuffle(seed=42).select(range(40))\n","specific_test_dataset = test_dataset_2[\"test\"]\n","eval_dataset_1 = specific_test_dataset.shuffle(seed=42).select(range(2))"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:24:19.449795Z","iopub.status.busy":"2024-04-29T01:24:19.449421Z","iopub.status.idle":"2024-04-29T01:24:19.475918Z","shell.execute_reply":"2024-04-29T01:24:19.475054Z","shell.execute_reply.started":"2024-04-29T01:24:19.449769Z"},"trusted":true},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir=\"llava-1.5-7b-hf-ft-mix-vsft-3\",\n","    report_to=\"tensorboard\",\n","    learning_rate=1.4e-5,\n","    per_device_train_batch_size=2,\n","    gradient_accumulation_steps=1,\n","    logging_steps=5,\n","    num_train_epochs=1,\n","    push_to_hub=True,\n","    gradient_checkpointing=True,\n","    remove_unused_columns=False,\n","    fp16=True,\n","    bf16=False\n",")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:24:22.198559Z","iopub.status.busy":"2024-04-29T01:24:22.197700Z","iopub.status.idle":"2024-04-29T01:24:22.203033Z","shell.execute_reply":"2024-04-29T01:24:22.201783Z","shell.execute_reply.started":"2024-04-29T01:24:22.198514Z"},"trusted":true},"outputs":[],"source":["lora_config = LoraConfig(\n","    r=64,\n","    lora_alpha=16,\n","    target_modules=\"all-linear\"\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:23:46.420745Z","iopub.status.busy":"2024-04-29T01:23:46.420110Z","iopub.status.idle":"2024-04-29T01:23:58.807766Z","shell.execute_reply":"2024-04-29T01:23:58.806512Z","shell.execute_reply.started":"2024-04-29T01:23:46.420713Z"},"trusted":true},"outputs":[],"source":["%%capture\n","!pip install huggingface_hub"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:23:58.809629Z","iopub.status.busy":"2024-04-29T01:23:58.809282Z","iopub.status.idle":"2024-04-29T01:23:58.837236Z","shell.execute_reply":"2024-04-29T01:23:58.836355Z","shell.execute_reply.started":"2024-04-29T01:23:58.809601Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a6e0e6580d34333866864052a165566","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:24:32.172194Z","iopub.status.busy":"2024-04-29T01:24:32.171439Z","iopub.status.idle":"2024-04-29T01:24:35.699966Z","shell.execute_reply":"2024-04-29T01:24:35.698992Z","shell.execute_reply.started":"2024-04-29T01:24:32.172159Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n","  warnings.warn(\n"]}],"source":["trainer = SFTTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset_1,\n","    eval_dataset=eval_dataset_1,\n","    peft_config=lora_config,\n","    dataset_text_field=\"text\",  # need a dummy field\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    dataset_kwargs={\"skip_prepare_dataset\": True},\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### **Training**"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:24:35.915790Z","iopub.status.busy":"2024-04-29T01:24:35.915427Z","iopub.status.idle":"2024-04-29T01:25:11.874064Z","shell.execute_reply":"2024-04-29T01:25:11.872992Z","shell.execute_reply.started":"2024-04-29T01:24:35.915765Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3/3 00:22, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n"]},{"data":{"text/plain":["TrainOutput(global_step=3, training_loss=0.9628692468007406, metrics={'train_runtime': 35.417, 'train_samples_per_second': 0.169, 'train_steps_per_second': 0.085, 'total_flos': 54532828078080.0, 'train_loss': 0.9628692468007406, 'epoch': 1.0})"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["### **Pushing to Hub**"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:25:19.137398Z","iopub.status.busy":"2024-04-29T01:25:19.136052Z","iopub.status.idle":"2024-04-29T01:26:06.907306Z","shell.execute_reply":"2024-04-29T01:26:06.906365Z","shell.execute_reply.started":"2024-04-29T01:25:19.137351Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n","  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f63eaa24c08b4e9580c5929b4b40895b","version_major":2,"version_minor":0},"text/plain":["training_args.bin:   0%|          | 0.00/4.98k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"22959a705b4b45cca0324a3014548703","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b0b5959760444d7a7d7983a017b2912","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/1.29G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9d15a6b5b3c4ad3abf0d06bbd8359a2","version_major":2,"version_minor":0},"text/plain":["Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98ac55023b0c453380d12d6aeb5e901d","version_major":2,"version_minor":0},"text/plain":["events.out.tfevents.1714353876.6e9d271be7ad.33.0:   0%|          | 0.00/5.87k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/Praveen0309/llava-1.5-7b-hf-ft-mix-vsft-3/commit/6188f30e2f215b54c428d2f5b6ce43f69eeae048', commit_message='End of training', commit_description='', oid='6188f30e2f215b54c428d2f5b6ce43f69eeae048', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["trainer.push_to_hub()"]},{"cell_type":"markdown","metadata":{},"source":["### **Inferring from Hub**"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:26:36.029074Z","iopub.status.busy":"2024-04-29T01:26:36.028037Z","iopub.status.idle":"2024-04-29T01:29:21.878358Z","shell.execute_reply":"2024-04-29T01:29:21.877329Z","shell.execute_reply.started":"2024-04-29T01:26:36.029031Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d3f1f6881b204564836a53808fe32049","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fde22b7e30a44a8a8f19044f672b96e8","version_major":2,"version_minor":0},"text/plain":["adapter_config.json:   0%|          | 0.00/941 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e227b75f91ef43069f4dcbb2169c8421","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/1.29G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"502a44daafda4f6e9e80ff6013bbedc7","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02e744d84135447cabcee2592de7942a","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8434b4433574a4991db9ff79c0f484d","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a830676129ae4085a6c3178c4c291152","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0ab6af7b6e484e71bdf9feaa44256e0d","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1b801878af0f478c9e610816ccc9922d","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1510: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Generated response: USER:  \n","What's the content of the image? ASSISTANT: The image features a stop sign on a city street, with a car driving\n"]}],"source":["from peft import PeftConfig, PeftModel\n","from PIL import Image\n","import requests\n","\n","# model_id = \"llava-hf/llava-1.5-7b-hf\"\n","model_id = \"HuggingFaceH4/vsft-llava-1.5-7b-hf-trl\"\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n",")\n","\n","base_model = LlavaForConditionalGeneration.from_pretrained(model_id,\n","                                                      quantization_config=quantization_config,\n","                                                      torch_dtype=torch.float16)\n","# Load the PEFT Lora model (adapter)\n","peft_lora_adapter_path = \"Praveen0309/llava-1.5-7b-hf-ft-mix-vsft-3\"\n","peft_lora_adapter = PeftModel.from_pretrained(base_model, peft_lora_adapter_path, adapter_name=\"lora_adapter\")\n","\n","# Merge the adapters into the base model\n","base_model.load_adapter(peft_lora_adapter_path, adapter_name=\"lora_adapter\")\n","\n","# Now you can use the combined model (base model + PEFT Lora adapter) for inference\n","# For example:\n","prompt = \"USER: <image>\\nWhat's the content of the image? ASSISTANT:\"\n","url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n","image = Image.open(requests.get(url, stream=True).raw)\n","# processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n","processor = AutoProcessor.from_pretrained(\"HuggingFaceH4/vsft-llava-1.5-7b-hf-trl\")\n","inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n","# ... process the image and create inputs ...\n","generate_ids = base_model.generate(**inputs, max_new_tokens=15)\n","decoded_response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n","print(\"Generated response:\", decoded_response)"]},{"cell_type":"markdown","metadata":{},"source":["### **Saving model locally**"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:29:44.385756Z","iopub.status.busy":"2024-04-29T01:29:44.384803Z","iopub.status.idle":"2024-04-29T01:29:46.461887Z","shell.execute_reply":"2024-04-29T01:29:46.460860Z","shell.execute_reply.started":"2024-04-29T01:29:44.385713Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n","  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"]}],"source":["base_model.save_pretrained(\"kaggle_model_3\")"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["### **Inferring Locally**"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T01:50:05.931880Z","iopub.status.busy":"2024-04-29T01:50:05.930805Z","iopub.status.idle":"2024-04-29T01:57:09.332053Z","shell.execute_reply":"2024-04-29T01:57:09.331035Z","shell.execute_reply.started":"2024-04-29T01:50:05.931824Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5fc375f139104c88a5a50a90dc7371b5","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/971 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66e0c528882b4e8eae55afb4223770db","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/70.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd067389be054cf79235f8c738526775","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c93d27d6d534803a21cd460921eb9fc","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00006.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c8955c0bffec4df4bc21abf4b393df72","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d112e423e8d4234808f6ff7767865e2","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f8b8052c1bc3457187fdb8d87efe00b2","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7fc2f4fd0f984ae08724a7a4df476f69","version_major":2,"version_minor":0},"text/plain":["model-00005-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"375281f8b377473197d91490bdeac45c","version_major":2,"version_minor":0},"text/plain":["model-00006-of-00006.safetensors:   0%|          | 0.00/3.94G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1aad585cf1654bdbb51611f994230023","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"388aeda6cb5e404796ee56f0dc3da9da","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/136 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cdb3177b8dfa46969d20ee7edb2ec50b","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/819 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"554a2482006c43688dee073ba5920557","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26d2f52674bd4efaa15e26a81d4227a1","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"63c0dd3735e044f0be6975c6a8b16697","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dabcf9f68457406ea281cf430e5fd350","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f748cd32ae049e1804035aa0616c08c","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1510: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Generated response: USER:  \n","What's the content of the image? ASSISTANT: The image features a stop sign on a city street, with a car driving\n"]}],"source":["from peft import PeftConfig, PeftModel\n","from PIL import Image\n","import requests\n","\n","# model_id = \"llava-hf/llava-1.5-7b-hf\"\n","model_id = r\"C:\\Users\\prave\\OneDrive\\Desktop\\MLOPS\\Mlops_2\\huggingface_model\"\n","base_model = LlavaForConditionalGeneration.from_pretrained(model_id)\n","\n","processor = AutoProcessor.from_pretrained(r\"C:\\Users\\prave\\OneDrive\\Desktop\\MLOPS\\Mlops_2\\huggingface_processor\")\n","\n","# Load the PEFT Lora model (adapter)\n","peft_lora_adapter_path = r\"C:\\Users\\prave\\OneDrive\\Desktop\\MLOPS\\Mlops_2\\huggingface_adapter\"\n","\n","# Load the PEFT Lora model (adapter)\n","peft_lora_adapter = PeftModel.from_pretrained(base_model, peft_lora_adapter_path, adapter_name=\"lora_adapter\")\n","\n","# Merge the adapters into the base model\n","base_model.load_adapter(peft_lora_adapter_path, adapter_name=\"lora_adapter\")\n","\n","# Now you can use the combined model (base model + PEFT Lora adapter) for inference\n","# For example:\n","prompt = \"USER: <image>\\nWhat's the content of the image? ASSISTANT:\"\n","# url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n","url = \"/kaggle/input/images/images/1921.428_web.jpg\"\n","image = Image.open(url)\n","# image = Image.open(requests.get(url, stream=True).raw)\n","# # processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n","inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n","# ... process the image and create inputs ...\n","generate_ids = base_model.generate(**inputs, max_new_tokens=15)\n","decoded_response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n","print(\"Generated response:\", decoded_response)"]},{"cell_type":"markdown","metadata":{},"source":["### **Translating**"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T02:12:12.403897Z","iopub.status.busy":"2024-04-29T02:12:12.403491Z","iopub.status.idle":"2024-04-29T02:12:12.592434Z","shell.execute_reply":"2024-04-29T02:12:12.591459Z","shell.execute_reply.started":"2024-04-29T02:12:12.403855Z"},"trusted":true},"outputs":[],"source":["from deep_translator import GoogleTranslator\n","def deep_translator_bn_en(input_sentence):\n","  english_translation = GoogleTranslator(source=\"bn\", target=\"en\").translate(input_sentence)\n","  return english_translation\n","\n","def deep_translator_en_bn(input_sentence):\n","  bengali_translation = GoogleTranslator(source=\"en\", target=\"bn\").translate(input_sentence)\n","  return bengali_translation\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n","tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n","\n","def facebook_response(url, input_sentence):\n","  url = input(\"à¦‡à¦®à§‡à¦œ url à¦²à¦¿à¦–à§à¦¨: \")\n","  input_sentence = input(\"à¦›à¦¬à¦¿ à¦¸à¦®à§à¦ªà¦°à§à¦•à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦ªà§à¦°à¦¶à§à¦¨ à¦²à¦¿à¦–à§à¦¨: \")\n","  image_prompt = facebook_bn_en(input_sentence)\n","  response = inference(image_prompt, url)\n","  assistant_index = response.find(\"ASSISTANT:\")\n","  extracted_string = response[assistant_index + len(\"ASSISTANT:\"):].strip()\n","  output = facebook_en_bn(extracted_string)\n","  print(\"à¦¬à¦Ÿà§€: \", output)\n","  return output\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T02:19:36.341687Z","iopub.status.busy":"2024-04-29T02:19:36.341351Z","iopub.status.idle":"2024-04-29T02:19:36.348524Z","shell.execute_reply":"2024-04-29T02:19:36.347437Z","shell.execute_reply.started":"2024-04-29T02:19:36.341662Z"},"trusted":true},"outputs":[],"source":["def inference(image_prompt, url):\n","    prompt = f\"USER: <image>\\n{image_prompt} ASSISTANT:\"\n","#     prompt = \"USER: <image>\\nWhat's the content of the image? ASSISTANT:\"\n","    # url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n","#     url = \"/kaggle/input/images/images/1921.428_web.jpg\"\n","    # image = Image.open(url)\n","    image = Image.open(requests.get(url, stream=True).raw)\n","    # processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n","    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n","    # ... process the image and create inputs ...\n","    generate_ids = base_model.generate(**inputs, max_new_tokens=15)\n","    decoded_response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n","#     print(\"Generated response:\", decoded_response)\n","    return decoded_response\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T02:23:41.650440Z","iopub.status.busy":"2024-04-29T02:23:41.649685Z","iopub.status.idle":"2024-04-29T02:23:45.420503Z","shell.execute_reply":"2024-04-29T02:23:45.419501Z","shell.execute_reply.started":"2024-04-29T02:23:41.650407Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["à¦›à¦¬à¦¿à¦¤à§‡ à¦¸à¦¾à¦¦à¦¾ à¦ªà§‹à¦¶à¦¾à¦• à¦ªà¦°à¦¾ à¦à¦•à¦œà¦¨ à¦®à¦¹à¦¿à¦²à¦¾ à¦šà§‡à¦¯à¦¼à¦¾à¦°à§‡ à¦¬à¦¸à§‡ à¦†à¦›à§‡à¦¨\n"]}],"source":["url = \"/kaggle/input/images/images/1921.428_web.jpg\"\n","input_sentence = \"à¦›à¦¬à¦¿à¦° à¦¬à¦¿à¦·à¦¯à¦¼à¦¬à¦¸à§à¦¤à§ à¦•à¦¿?\"\n","image_prompt = deep_translator_bn_en(input_sentence)\n","response = inference(image_prompt, url)\n","assistant_index = response.find(\"ASSISTANT:\")\n","extracted_string = response[assistant_index + len(\"ASSISTANT:\"):].strip()\n","output = deep_translator_en_bn(extracted_string)\n","print(output)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T02:26:08.209788Z","iopub.status.busy":"2024-04-29T02:26:08.209407Z","iopub.status.idle":"2024-04-29T02:26:28.921268Z","shell.execute_reply":"2024-04-29T02:26:28.920350Z","shell.execute_reply.started":"2024-04-29T02:26:08.209758Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["à¦‡à¦®à§‡à¦œ url à¦²à¦¿à¦–à§à¦¨ /kaggle/input/images/images/1921.428_web.jpg\n","à¦›à¦¬à¦¿ à¦¸à¦®à§à¦ªà¦°à§à¦•à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦ªà§à¦°à¦¶à§à¦¨ à¦²à¦¿à¦–à§à¦¨ à¦›à¦¬à¦¿à¦° à¦¬à¦¿à¦·à¦¯à¦¼à¦¬à¦¸à§à¦¤à§ à¦•à¦¿?\n"]},{"name":"stdout","output_type":"stream","text":["à¦¬à¦Ÿà§€:  à¦›à¦¬à¦¿à¦¤à§‡ à¦¸à¦¾à¦¦à¦¾ à¦ªà§‹à¦¶à¦¾à¦• à¦ªà¦°à¦¾ à¦à¦•à¦œà¦¨ à¦®à¦¹à¦¿à¦²à¦¾ à¦šà§‡à¦¯à¦¼à¦¾à¦°à§‡ à¦¬à¦¸à§‡ à¦†à¦›à§‡à¦¨\n"]}],"source":["url = input(\"à¦‡à¦®à§‡à¦œ url à¦²à¦¿à¦–à§à¦¨: \")\n","input_sentence = input(\"à¦›à¦¬à¦¿ à¦¸à¦®à§à¦ªà¦°à§à¦•à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦ªà§à¦°à¦¶à§à¦¨ à¦²à¦¿à¦–à§à¦¨: \")\n","image_prompt = deep_translator_bn_en(input_sentence)\n","response = inference(image_prompt, url)\n","assistant_index = response.find(\"ASSISTANT:\")\n","extracted_string = response[assistant_index + len(\"ASSISTANT:\"):].strip()\n","output = deep_translator_en_bn(extracted_string)\n","print(\"à¦¬à¦Ÿà§€: \", output)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["url = input(\"à¦‡à¦®à§‡à¦œ url à¦²à¦¿à¦–à§à¦¨: \")\n","input_sentence = input(\"à¦›à¦¬à¦¿ à¦¸à¦®à§à¦ªà¦°à§à¦•à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦ªà§à¦°à¦¶à§à¦¨ à¦²à¦¿à¦–à§à¦¨: \")\n","image_prompt = facebook_translator_bn_en(input_sentence)\n","response = inference(image_prompt, url)\n","assistant_index = response.find(\"ASSISTANT:\")\n","extracted_string = response[assistant_index + len(\"ASSISTANT:\"):].strip()\n","output = facebook_translator_en_bn(extracted_string)\n","print(\"à¦¬à¦Ÿà§€: \", output)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4891853,"sourceId":8245500,"sourceType":"datasetVersion"},{"datasetId":4897351,"sourceId":8253161,"sourceType":"datasetVersion"},{"datasetId":4897354,"sourceId":8253166,"sourceType":"datasetVersion"},{"datasetId":4900529,"sourceId":8257374,"sourceType":"datasetVersion"},{"datasetId":4900533,"sourceId":8257380,"sourceType":"datasetVersion"},{"datasetId":4900564,"sourceId":8257436,"sourceType":"datasetVersion"},{"datasetId":4900566,"sourceId":8257438,"sourceType":"datasetVersion"},{"datasetId":4900806,"sourceId":8257770,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
